{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu50OAk62Zkp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "4094001f-3c77-4ab7-8309-435771da3e26"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import unicodedata, re, string\n",
        "import nltk\n",
        "nltk.download()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(color_codes=True)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "train = pd.read_csv(\"https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv\", sep=\"\\t\")\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/test.tsv\", sep=\"\\t\")\n",
        "\n",
        "def remove_non_ascii(words):\n",
        "    #Remove non-ASCII characters\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    #Convert all characters to lowercase \n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    #Remove punctuation \n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_numbers(words):\n",
        "   # Remove all interger occurrences \n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(\"\\d+\", \"\", word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    #Remove stop words \n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def stem_words(words):\n",
        "    #Stem words \n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "  #Lemmatize verbs\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_numbers(words)\n",
        "#    words = remove_stopwords(words)\n",
        "    return words\n",
        "\n",
        "train['Words'] = train['Phrase'].apply(nltk.word_tokenize)\n",
        "\n",
        "\n",
        "train['Words'] = train['Words'].apply(normalize) \n",
        "train['Words'].head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [a, series, of, escapades, demonstrating, the,...\n",
              "1    [a, series, of, escapades, demonstrating, the,...\n",
              "2                                          [a, series]\n",
              "3                                                  [a]\n",
              "4                                             [series]\n",
              "Name: Words, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhTs-zK2BKOY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bb9cbacb-ccfb-45c5-8b18-7ca3f57c8940"
      },
      "source": [
        "word_set = set()\n",
        "for l in train['Words']:\n",
        "    for e in l:\n",
        "        word_set.add(e)\n",
        "        \n",
        "word_to_int = {word: ii for ii, word in enumerate(word_set, 1)}\n",
        "\n",
        "print(len(word_set))\n",
        "print(len(word_to_int))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16209\n",
            "16209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg75F50keuqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "4091ebba-a2df-4ec0-ad1b-f05ae3871a98"
      },
      "source": [
        "train['Tokens'] = train['Words'].apply(lambda l: [word_to_int[word] for word in l])\n",
        "train['Tokens'].head()\n",
        "\n",
        "max_len = train['Tokens'].str.len().max()\n",
        "print(max_len)\n",
        "\n",
        "all_tokens = np.array([t for t in train['Tokens']])\n",
        "encoded_labels = np.array([l for l in train['Sentiment']])\n",
        "\n",
        "# Create blank rows\n",
        "features = np.zeros((len(all_tokens), max_len), dtype=int)\n",
        "# for each phrase, add zeros at the end \n",
        "for i, row in enumerate(all_tokens):\n",
        "    features[i, :len(row)] = row\n",
        "\n",
        "#print first 3 values of the feature matrix \n",
        "print(features[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48\n",
            "[[14089  6441 10895 14987  9491   739  3433 10832 13872  9256   515  3575\n",
            "    739  2552  9256 14389   515  3575   739   843 14534 10895 10932 14136\n",
            "   1356  3965  2893 10895 10932  6814  4638  1673 10895 14089  4667     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [14089  6441 10895 14987  9491   739  3433 10832 13872  9256   515  3575\n",
            "    739  2552     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [14089  6441     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtoTU_MaBYmC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "ba676ec6-47a1-4684-f3b1-3cc6dfcb8cbc"
      },
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data\n",
        "\n",
        "split_idx = int(len(features)*0.8)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of  resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(124848, 48) \n",
            "Validation set: \t(15606, 48) \n",
            "Test set: \t\t(15606, 48)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQzSr-wfBzrl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4d7f4f37-ddfe-434b-be38-103af7ba9c92"
      },
      "source": [
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "\n",
        "batch_size = 54\n",
        "\n",
        "#  SHUFFLE training data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(valid_loader))\n",
        "print(len(test_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2312\n",
            "289\n",
            "289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysppjFLMEb0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "13c51023-d080-45e0-c72f-a0b872b24a56"
      },
      "source": [
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfJdADCXCGoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    \n",
        "  #The RNN model \n",
        "    \n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \n",
        "       #Initialize the model by setting up the layers.\n",
        "        \n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        \n",
        "        #Perform a forward pass \n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        # transform lstm output to input size of linear layers\n",
        "        lstm_out = lstm_out.transpose(0,1)\n",
        "        lstm_out = lstm_out[-1]\n",
        "\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)        \n",
        "\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "       # Initializes hidden state '''\n",
        "    \n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ayn7tKSvCSkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "82f1611f-8ebe-4185-e656-68e4c0631d5f"
      },
      "source": [
        "vocab_size = len(word_to_int)+1 # +1 for the 0 padding\n",
        "output_size = 5\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(16210, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUlqry-kDnoF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "57a1bf7e-7aeb-422f-b963-c9bb485c49c9"
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.003\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "# training params\n",
        "epochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "       \n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        \n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "              \n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output, labels)\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/3... Step: 100... Loss: 1.355597... Val Loss: 1.304309\n",
            "Epoch: 1/3... Step: 200... Loss: 1.344680... Val Loss: 1.307596\n",
            "Epoch: 1/3... Step: 300... Loss: 1.234210... Val Loss: 1.308666\n",
            "Epoch: 1/3... Step: 400... Loss: 1.304678... Val Loss: 1.301185\n",
            "Epoch: 1/3... Step: 500... Loss: 1.297138... Val Loss: 1.310768\n",
            "Epoch: 1/3... Step: 600... Loss: 1.532255... Val Loss: 1.302751\n",
            "Epoch: 1/3... Step: 700... Loss: 1.288445... Val Loss: 1.302777\n",
            "Epoch: 1/3... Step: 800... Loss: 1.343624... Val Loss: 1.309740\n",
            "Epoch: 1/3... Step: 900... Loss: 1.284488... Val Loss: 1.302486\n",
            "Epoch: 1/3... Step: 1000... Loss: 1.195354... Val Loss: 1.303932\n",
            "Epoch: 1/3... Step: 1100... Loss: 1.334542... Val Loss: 1.305860\n",
            "Epoch: 1/3... Step: 1200... Loss: 1.220833... Val Loss: 1.302766\n",
            "Epoch: 1/3... Step: 1300... Loss: 1.268051... Val Loss: 1.301523\n",
            "Epoch: 1/3... Step: 1400... Loss: 1.185577... Val Loss: 1.254699\n",
            "Epoch: 1/3... Step: 1500... Loss: 1.323206... Val Loss: 1.254830\n",
            "Epoch: 1/3... Step: 1600... Loss: 1.261424... Val Loss: 1.243292\n",
            "Epoch: 1/3... Step: 1700... Loss: 1.183355... Val Loss: 1.230437\n",
            "Epoch: 1/3... Step: 1800... Loss: 1.247302... Val Loss: 1.222592\n",
            "Epoch: 1/3... Step: 1900... Loss: 1.110520... Val Loss: 1.212289\n",
            "Epoch: 1/3... Step: 2000... Loss: 1.158289... Val Loss: 1.218025\n",
            "Epoch: 1/3... Step: 2100... Loss: 1.416330... Val Loss: 1.197923\n",
            "Epoch: 1/3... Step: 2200... Loss: 1.279612... Val Loss: 1.196525\n",
            "Epoch: 1/3... Step: 2300... Loss: 1.126784... Val Loss: 1.186203\n",
            "Epoch: 2/3... Step: 2400... Loss: 0.942315... Val Loss: 1.184121\n",
            "Epoch: 2/3... Step: 2500... Loss: 0.971384... Val Loss: 1.197234\n",
            "Epoch: 2/3... Step: 2600... Loss: 1.185804... Val Loss: 1.185220\n",
            "Epoch: 2/3... Step: 2700... Loss: 1.069558... Val Loss: 1.165464\n",
            "Epoch: 2/3... Step: 2800... Loss: 1.081835... Val Loss: 1.160550\n",
            "Epoch: 2/3... Step: 2900... Loss: 1.202025... Val Loss: 1.152142\n",
            "Epoch: 2/3... Step: 3000... Loss: 0.960220... Val Loss: 1.144732\n",
            "Epoch: 2/3... Step: 3100... Loss: 0.893339... Val Loss: 1.144284\n",
            "Epoch: 2/3... Step: 3200... Loss: 1.000495... Val Loss: 1.145125\n",
            "Epoch: 2/3... Step: 3300... Loss: 1.193499... Val Loss: 1.140934\n",
            "Epoch: 2/3... Step: 3400... Loss: 1.048567... Val Loss: 1.145275\n",
            "Epoch: 2/3... Step: 3500... Loss: 1.128055... Val Loss: 1.136472\n",
            "Epoch: 2/3... Step: 3600... Loss: 1.052679... Val Loss: 1.141145\n",
            "Epoch: 2/3... Step: 3700... Loss: 1.089087... Val Loss: 1.133102\n",
            "Epoch: 2/3... Step: 3800... Loss: 0.983540... Val Loss: 1.126166\n",
            "Epoch: 2/3... Step: 3900... Loss: 0.987568... Val Loss: 1.115562\n",
            "Epoch: 2/3... Step: 4000... Loss: 1.084506... Val Loss: 1.117872\n",
            "Epoch: 2/3... Step: 4100... Loss: 0.985257... Val Loss: 1.111071\n",
            "Epoch: 2/3... Step: 4200... Loss: 0.977513... Val Loss: 1.120095\n",
            "Epoch: 2/3... Step: 4300... Loss: 1.146915... Val Loss: 1.122215\n",
            "Epoch: 2/3... Step: 4400... Loss: 1.024705... Val Loss: 1.111528\n",
            "Epoch: 2/3... Step: 4500... Loss: 1.090466... Val Loss: 1.124803\n",
            "Epoch: 2/3... Step: 4600... Loss: 1.134842... Val Loss: 1.110080\n",
            "Epoch: 3/3... Step: 4700... Loss: 0.888049... Val Loss: 1.116797\n",
            "Epoch: 3/3... Step: 4800... Loss: 0.994470... Val Loss: 1.134604\n",
            "Epoch: 3/3... Step: 4900... Loss: 0.976643... Val Loss: 1.123843\n",
            "Epoch: 3/3... Step: 5000... Loss: 0.911411... Val Loss: 1.130942\n",
            "Epoch: 3/3... Step: 5100... Loss: 1.114531... Val Loss: 1.100378\n",
            "Epoch: 3/3... Step: 5200... Loss: 1.019306... Val Loss: 1.121025\n",
            "Epoch: 3/3... Step: 5300... Loss: 1.152619... Val Loss: 1.131246\n",
            "Epoch: 3/3... Step: 5400... Loss: 0.988771... Val Loss: 1.121466\n",
            "Epoch: 3/3... Step: 5500... Loss: 0.855077... Val Loss: 1.108152\n",
            "Epoch: 3/3... Step: 5600... Loss: 1.012211... Val Loss: 1.111740\n",
            "Epoch: 3/3... Step: 5700... Loss: 0.635174... Val Loss: 1.097817\n",
            "Epoch: 3/3... Step: 5800... Loss: 1.004119... Val Loss: 1.099772\n",
            "Epoch: 3/3... Step: 5900... Loss: 0.986127... Val Loss: 1.079443\n",
            "Epoch: 3/3... Step: 6000... Loss: 1.069300... Val Loss: 1.085957\n",
            "Epoch: 3/3... Step: 6100... Loss: 1.023220... Val Loss: 1.083681\n",
            "Epoch: 3/3... Step: 6200... Loss: 0.957375... Val Loss: 1.094231\n",
            "Epoch: 3/3... Step: 6300... Loss: 0.717920... Val Loss: 1.103383\n",
            "Epoch: 3/3... Step: 6400... Loss: 0.889858... Val Loss: 1.092040\n",
            "Epoch: 3/3... Step: 6500... Loss: 1.011189... Val Loss: 1.071114\n",
            "Epoch: 3/3... Step: 6600... Loss: 0.825855... Val Loss: 1.085897\n",
            "Epoch: 3/3... Step: 6700... Loss: 0.941566... Val Loss: 1.068217\n",
            "Epoch: 3/3... Step: 6800... Loss: 0.899502... Val Loss: 1.089803\n",
            "Epoch: 3/3... Step: 6900... Loss: 0.928829... Val Loss: 1.081444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V14_fAuREvTW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7ae619b1-9bb2-472c-c4d4-1eebebe55664"
      },
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    \n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output, labels)\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    \n",
        "    _, pred = torch.max(output,1)\n",
        "    \n",
        "    \n",
        "    correct_tensor = pred.eq(labels.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.081\n",
            "Test accuracy: 0.565\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}